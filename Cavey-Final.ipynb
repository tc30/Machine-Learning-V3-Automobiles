{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*by Tom Cavey, May 9, 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In my project, I will try to predict the vehicle manufactuerer of a car pictured. I am using a Transfer learning model for image classification from Google called Inception-V3. This pre-trained model is not specific to recognizing cars, but I hope to test many pre-trained networks and compare them.\n",
    "\n",
    "Transfer learning is where you take a neural network that has already been trained for a different task, and tweak it to solve a new problem or task. Because I want to modify the network to meet the needs of my dataset, the last layer (or more) of the pre-trained network will be removed and replaced with my own classifier. It is important that the weights of the layers that preceed the last one are not changed.\n",
    "\n",
    "This is much easier than building a network from scratch, like I originally begun to do (See \"Things that went wrong\" section). With Transfer learning the key to making the pre-trained model work with a new dataset is to fine-tune the model. This can be tricky, because if the previous task that the network was originally trained and used for is too different than the problem I am trying to solve, it could result in not being able to properly classify the data.\n",
    "In building the new model with Transfer learning since most of it is made already, you can train your model using fewer computational resources and a reduction training time.\n",
    "\n",
    "It is obvious why using a Transfer learning network is much easier than building a new network from scratch. However, just because it is easier does not mean that it will be more accurate. This is the downside of using a Trasnfer learning network that has been pre-trained on other data. So, keeping this in mind, and not being able to successfully build a convolutional neural net from scratch- I will set my goals to try and test multiple different Transfer learning networks and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vehicle Data set\n",
    "\n",
    "First I wanted to find the proper data set. Initially, all that seemed to be available were low quality images of the rear end of some vehicles which were found in some computer vision Github repositories. This wouldn't do because I needed images that were from all sides fo the vehicles, and in high resolution. (It wasn't until later that I realized raw high res images probably arent the best suited for me). I did find a Cifar dataset that seemed OK, but I later stumbled upon a webpage from Jonathan Krause, a Computer Science student at Standford University who did a similar experiment to mine- where all the images had been made available to download. To my suprise, this data set had nice high resolution images of all makes of cars. Unfortuneatly they weren't yet divided into classification folders. I had to do this manually. So I went through the 16,000 image database and seperated the cars by manufactuerer into classification folders. Turned out to be fairly simple because they were ordered in descending alphabetical order by manufactuerer (thank you, Jonathan.)\n",
    "\n",
    "Once I got the images in their seperate classification folders, I created the testing image folder. I went through each of the manufactueres and selected some cars from the original train batch. I had planned to use some of my own pictures I took at car meets and pictures of my own car. I had to make sure I did not include pictures of cars that were not in the training set, but I thought it would be funny to see what a Subaru would be identified as, which is not in my original training set. (more on this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the Transfer Learning Network\n",
    "First thing I had to do was set up the Transfer Learning network to use my data set. This was not as straight forward as I might make it seem, but after playing around with it for a few days I finally figured it out. This led me to realize that I needed to resize my images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing the images\n",
    "Tensorflow needs to have the images be resized to a 299 x 299 square. The original code in the IBM PowerAI Transfer learning Github had a method which took care of this resizing for me. It takes the raw images from my directory with all the classification folders and resizes them into a new directory, cool!\n",
    "\n",
    "#### For example, this was a raw data image (full res):\n",
    "\n",
    "<img src=\"015846.jpg\">\n",
    "\n",
    "#### To this 299 x 299 image: \n",
    "\n",
    "<img src=\"sample_images/smallC30.jpg\">\n",
    "\n",
    "Using the following command, I was able to install the needed Python package to resize the images. The cell below only needs to be run once, and the notebook kernel needs to be restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organizing the images by classification\n",
    "This part I had to do manually, moving each batch of vehicle into its correct folder. The CS445_images folder contains jpeg images in a directory of sub folders with each labeled as \"Acura\", \"Volvo\", \"Ferrari\", etc. The subfolder names are crucial because that's how TensorFlow knows which classification label belongs to which image, it uses the subfolder's name as the classification label for the images which are contained within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This was used to install the package necessary to resize the images.\n",
    "#!pip install --user python-resize-image==1.1.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Import statements. Taken directly from the IBM PowerAI Github repo. (note: no PowerAI was used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Boulderc30/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "from PIL import Image\n",
    "from resizeimage import resizeimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import graph_util\n",
    "from tensorflow.python.platform import gfile\n",
    "\n",
    "module_path = os.path.abspath('..')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from image_retraining import retrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the images\n",
    "The Inception model requires 299 X 299 pixel sizes.\n",
    "First copy the files from `stored_images_resized` into `images_resized_dir`.\n",
    "With these stored images that are already resized, we don't need to repeat the process.\n",
    "Next copy and resize the remaining raw images from `image_dir` into `images_resized_dir`.\n",
    "\n",
    "We don't need to use any of the code below, as I already have done this step in the process. I commented it out because I would like to continue using it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def resize_images(src_dir, dest_dir):\n",
    "#     if not os.path.isdir(src_dir):\n",
    "#         raise Exception(src_dir + \" is not a directory\")\n",
    "#     if not os.path.exists(dest_dir):\n",
    "#         os.mkdir(dest_dir)\n",
    "\n",
    "#     raw_images = {image for image in os.listdir(src_dir) if image.endswith(\n",
    "#         JPEG_EXTENSIONS)}\n",
    "#     dest_images = {image for image in os.listdir(dest_dir)}\n",
    "\n",
    "#     # Resize the ones that are not already in the dest dir\n",
    "#     for image in raw_images - dest_images:\n",
    "#         if DEBUG:\n",
    "#             print(\"Resizing \" + image)\n",
    "#         resize_image(image, src_dir, dest_dir)\n",
    "\n",
    "# def resize_image(image_file, src_dir, dest_dir):\n",
    "#     in_file = os.path.join(src_dir, image_file)\n",
    "#     with open(in_file, 'r+b') as fd_img:\n",
    "#         with Image.open(fd_img) as img:\n",
    "#             resized_image = resizeimage.resize_contain(\n",
    "#                 img, [299, 299]).convert(\"RGB\")\n",
    "#             resized_image.save(os.path.join(dest_dir, image_file), img.format)\n",
    "\n",
    "# # Use a fresh working dir for the resized images\n",
    "# if os.path.isdir(images_resized_dir):\n",
    "#     shutil.rmtree(images_resized_dir)\n",
    "# os.mkdir(images_resized_dir)\n",
    "    \n",
    "# subdirs = ('Acura', 'Aston_Martin', 'Audi', 'Bentley'\n",
    "#           , 'BMW', 'Bugatti', 'Buick', 'Cadillac', 'Chevy', 'Chrystler'\n",
    "#           , 'Daewoo', 'Dodge', 'Ferrari', 'Fiat', 'Fisker', 'Ford'\n",
    "#            , 'Geo', 'GMC', 'Honda', 'Hummer', 'Hyundai', 'Infiniti'\n",
    "#           , 'Isuzu', 'Jaguar', 'Jeep', 'Lamborghini', 'Land_Rover'\n",
    "#           , 'Lincoln', 'Maybach', 'Mazda', 'Mclaren', 'Mercedes_Benz'\n",
    "#           , 'Mini', 'Mitsubishi', 'Nissan', 'Porsche', 'Rolls_Royce'\n",
    "#           , 'Scion', 'Smart', 'Spyker', 'Suzuki', 'Tesla', 'Toyota'\n",
    "#           , 'Volkswagen', 'Volvo')\n",
    "\n",
    "# # Copy in the image files, 'Suzuki'\n",
    "# for subdir in subdirs:\n",
    "#     dest_dir = os.path.join(images_resized_dir, subdir)\n",
    "#     if not os.path.isdir(dest_dir):\n",
    "#         os.mkdir(dest_dir)\n",
    "      \n",
    "#     # Copy the already resized files first, if any, from the repo or a custom dir\n",
    "#     if stored_images_resized:\n",
    "#         source_dir = os.path.join(stored_images_resized, subdir)\n",
    "#         if os.path.isdir(source_dir):\n",
    "#             for f in os.listdir(source_dir):\n",
    "#                 path = os.path.join(source_dir, f)\n",
    "#                 if (os.path.isfile(path)):\n",
    "#                     shutil.copy(path, dest_dir)\n",
    "                    \n",
    "#     # Copy/resize the remaining raw images into the images_resized_dir(s)\n",
    "#     resize_images(os.path.join(image_dir, subdir), dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the bottleneck producing code for the supplied images. This is commented out as it only needs to calculate the bottlenecks once (unless I add more images). This takes a really long time to run, don't run it unless you really mean to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a fresh working dir for the bottleneck files  \n",
    "# if os.path.isdir(bottleneck_dir):    \n",
    "#     shutil.rmtree(bottleneck_dir)\n",
    "# os.mkdir(bottleneck_dir)\n",
    "\n",
    "# subdirs = ('Acura', 'Aston_Martin', 'Audi', 'Bentley'\n",
    "#           , 'BMW', 'Bugatti', 'Buick', 'Cadillac', 'Chevy', 'Chrystler'\n",
    "#           , 'Daewoo', 'Dodge', 'Ferrari', 'Fiat', 'Fisker', 'Ford'\n",
    "#            , 'Geo', 'GMC', 'Honda', 'Hummer', 'Hyundai', 'Infiniti'\n",
    "#           , 'Isuzu', 'Jaguar', 'Jeep', 'Lamborghini', 'Land_Rover'\n",
    "#           , 'Lincoln', 'Maybach', 'Mazda', 'Mclaren', 'Mercedes_Benz'\n",
    "#           , 'Mini', 'Mitsubishi', 'Nissan', 'Porsche', 'Rolls_Royce'\n",
    "#           , 'Scion', 'Smart', 'Spyker', 'Suzuki', 'Tesla', 'Toyota'\n",
    "#           , 'Volkswagen', 'Volvo')\n",
    "\n",
    "# # Copy in the stored bottleneck files\n",
    "# for subdir in subdirs:\n",
    "#     dest_dir = os.path.join(bottleneck_dir, subdir)\n",
    "#     if not os.path.isdir(dest_dir):\n",
    "#         os.mkdir(dest_dir)\n",
    "\n",
    "#     image_dest_dir = os.path.join(images_resized_dir, subdir)\n",
    "\n",
    "#     if stored_bottlenecks:\n",
    "#         source_dir = os.path.join(stored_bottlenecks, subdir)\n",
    "#         if os.path.isdir(source_dir):\n",
    "#             for f in os.listdir(source_dir):\n",
    "#                 path = os.path.join(source_dir, f)\n",
    "#                 if (os.path.isfile(path)):\n",
    "#                     # Copy the persisted bottleneck to bottlenecks dir\n",
    "#                     shutil.copy(path, dest_dir)\n",
    "#                     # \"touch\" the file (w/o the .txt) to create a placeholder image\n",
    "#                     # This image file will only be used to build the lists.\n",
    "#                     if DEBUG:\n",
    "#                         print(\"Creating placeholder image at %s\" % os.path.join(image_dest_dir, f[:-4]))\n",
    "#                     open(os.path.join(image_dest_dir, f[:-4]), 'a').close\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "This is where I changed the parameters from the old Transfer learning network, to use my data set. I removed the original image_dir (the one which contained all the raw images) because the file was roughly 1.99GB in size. The compressed and resized version of the image_dir is much smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set DEBUG to True for more output\n",
    "DEBUG = False\n",
    "\n",
    "# Expect image files to always end with one of these\n",
    "JPEG_EXTENSIONS = ('.jpeg', '.JPEG', '.jpg', '.JPG')\n",
    "\n",
    "# Raw input images come from this dir in the git repo (or you can customize this to point to a new dir).\n",
    "# Only JPEG images are used. We will resize these images before using them.\n",
    "#image_dir = '../data/CS445_images'\n",
    "\n",
    "# We kept some images separate for our manual testing at the end.\n",
    "test_images_dir = 'newTest'\n",
    "\n",
    "# If stored_images_resized, images here have already been resized are can be used w/o re-resizing\n",
    "stored_images_resized = 'images_resized'  # set to None to ignore\n",
    "\n",
    "# If stored_bottlenecks, supplement the image_dir collection with persisted bottlenecks from this dir\n",
    "stored_bottlenecks = 'bottlenecks'  # set to None to ignore\n",
    "\n",
    "# Working files are in /tmp by default\n",
    "# tmp_dir = '/tmp'\n",
    "bottleneck_dir = os.path.join('bottlenecks')\n",
    "images_resized_dir = os.path.join('images_resized')\n",
    "summaries_dir = os.path.join('retrain_logs')\n",
    "\n",
    "# Download the original inception model to/from here\n",
    "model_dir = os.path.join('inception')\n",
    "inception_url = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "\n",
    "# Store the graph before and after training\n",
    "output_graph_orig = \"output_graph_orig.pb\"\n",
    "output_graph = \"output_graph.pb\"\n",
    "output_labels = \"output_labels.txt\"\n",
    "\n",
    "#Total number of misclassified images\n",
    "total = 0\n",
    "\n",
    "# Training params\n",
    "architecture = 'inception_v3'\n",
    "final_tensor_name = \"final_result\"\n",
    "how_many_training_steps = 700\n",
    "learning_rate = 0.2\n",
    "testing_percentage = 20\n",
    "validation_percentage = 80\n",
    "eval_step_interval = 10\n",
    "train_batch_size = 100\n",
    "test_batch_size = 200\n",
    "validation_batch_size = 100\n",
    "print_misclassified_test_images = True\n",
    "\n",
    "# Create a FLAGS object with these attributes\n",
    "FLAGS = type('FlagsObject', (object,), {\n",
    "    'architecture': architecture,\n",
    "    'model_dir': model_dir,\n",
    "    'intermediate_store_frequency': 0,\n",
    "    'summaries_dir': summaries_dir,\n",
    "    'learning_rate': learning_rate,\n",
    "    'image_dir': images_resized_dir,\n",
    "    'testing_percentage': testing_percentage,\n",
    "    'validation_percentage': validation_percentage,\n",
    "    'bottleneck_dir': bottleneck_dir,\n",
    "    'final_tensor_name': final_tensor_name,\n",
    "    'how_many_training_steps': how_many_training_steps,\n",
    "    'train_batch_size': train_batch_size,\n",
    "    'test_batch_size': test_batch_size,\n",
    "    'eval_step_interval': eval_step_interval,\n",
    "    'validation_batch_size': validation_batch_size,\n",
    "    'print_misclassified_test_images': print_misclassified_test_images,\n",
    "    'output_graph': output_graph,\n",
    "    'output_labels': output_labels\n",
    "})\n",
    "\n",
    "# Setting the FLAGS in retrain allows us to call the functions directly\n",
    "retrain.FLAGS = FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one peice of the original project that I kept, which is incredibly useful. The original creator had a FLAGS table which contained the variables used throughout the other fucntions. I will definitely be doing this in the future using Jupyter Notebooks as it was very helpful for keeping things in one place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for images in 'Acura'\n",
      "Looking for images in 'Aston_Martin'\n",
      "Looking for images in 'Audi'\n",
      "Looking for images in 'Bentley'\n",
      "Looking for images in 'BMW'\n",
      "Looking for images in 'Bugatti'\n",
      "Looking for images in 'Buick'\n",
      "Looking for images in 'Cadillac'\n",
      "Looking for images in 'Chevy'\n",
      "Looking for images in 'Chrystler'\n",
      "Looking for images in 'Daewoo'\n",
      "Looking for images in 'Dodge'\n",
      "Looking for images in 'Ferrari'\n",
      "Looking for images in 'Fiat'\n",
      "Looking for images in 'Fisker'\n",
      "Looking for images in 'Ford'\n",
      "Looking for images in 'Geo'\n",
      "Looking for images in 'GMC'\n",
      "Looking for images in 'Honda'\n",
      "Looking for images in 'Hummer'\n",
      "Looking for images in 'Hyundai'\n",
      "Looking for images in 'Infiniti'\n",
      "Looking for images in 'Isuzu'\n",
      "Looking for images in 'Jaguar'\n",
      "Looking for images in 'Jeep'\n",
      "Looking for images in 'Lamborghini'\n",
      "Looking for images in 'Land_Rover'\n",
      "Looking for images in 'Lincoln'\n",
      "Looking for images in 'Maybach'\n",
      "Looking for images in 'Mazda'\n",
      "Looking for images in 'Mclaren'\n",
      "Looking for images in 'Mercedes_Benz'\n",
      "Looking for images in 'Mini'\n",
      "Looking for images in 'Mitsubishi'\n",
      "Looking for images in 'Nissan'\n",
      "Looking for images in 'Porsche'\n",
      "Looking for images in 'Rolls_Royce'\n",
      "Looking for images in 'Scion'\n",
      "Looking for images in 'Smart'\n",
      "Looking for images in 'Spyker'\n",
      "Looking for images in 'Suzuki'\n",
      "Looking for images in 'Tesla'\n",
      "Looking for images in 'Toyota'\n",
      "Looking for images in 'Volkswagen'\n",
      "Looking for images in 'Volvo'\n",
      "100 bottleneck files created.\n",
      "200 bottleneck files created.\n",
      "300 bottleneck files created.\n",
      "400 bottleneck files created.\n",
      "500 bottleneck files created.\n",
      "600 bottleneck files created.\n",
      "700 bottleneck files created.\n",
      "800 bottleneck files created.\n",
      "900 bottleneck files created.\n",
      "1000 bottleneck files created.\n",
      "1100 bottleneck files created.\n",
      "1200 bottleneck files created.\n",
      "1300 bottleneck files created.\n",
      "1400 bottleneck files created.\n",
      "1500 bottleneck files created.\n",
      "1600 bottleneck files created.\n",
      "1700 bottleneck files created.\n",
      "1800 bottleneck files created.\n",
      "1900 bottleneck files created.\n",
      "2000 bottleneck files created.\n",
      "2100 bottleneck files created.\n",
      "2200 bottleneck files created.\n",
      "2300 bottleneck files created.\n",
      "2400 bottleneck files created.\n",
      "2500 bottleneck files created.\n",
      "2600 bottleneck files created.\n",
      "2700 bottleneck files created.\n",
      "2800 bottleneck files created.\n",
      "2900 bottleneck files created.\n",
      "3000 bottleneck files created.\n",
      "3100 bottleneck files created.\n",
      "3200 bottleneck files created.\n",
      "3300 bottleneck files created.\n",
      "3400 bottleneck files created.\n",
      "3500 bottleneck files created.\n",
      "3600 bottleneck files created.\n",
      "3700 bottleneck files created.\n",
      "3800 bottleneck files created.\n",
      "3900 bottleneck files created.\n",
      "4000 bottleneck files created.\n",
      "4100 bottleneck files created.\n",
      "4200 bottleneck files created.\n",
      "4300 bottleneck files created.\n",
      "4400 bottleneck files created.\n",
      "4500 bottleneck files created.\n",
      "4600 bottleneck files created.\n",
      "4700 bottleneck files created.\n",
      "4800 bottleneck files created.\n",
      "4900 bottleneck files created.\n",
      "5000 bottleneck files created.\n",
      "5100 bottleneck files created.\n",
      "5200 bottleneck files created.\n",
      "5300 bottleneck files created.\n",
      "5400 bottleneck files created.\n",
      "5500 bottleneck files created.\n",
      "5600 bottleneck files created.\n",
      "5700 bottleneck files created.\n",
      "5800 bottleneck files created.\n",
      "5900 bottleneck files created.\n",
      "6000 bottleneck files created.\n",
      "6100 bottleneck files created.\n",
      "6200 bottleneck files created.\n",
      "6300 bottleneck files created.\n",
      "6400 bottleneck files created.\n",
      "6500 bottleneck files created.\n",
      "6600 bottleneck files created.\n",
      "6700 bottleneck files created.\n",
      "6800 bottleneck files created.\n",
      "6900 bottleneck files created.\n",
      "7000 bottleneck files created.\n",
      "7100 bottleneck files created.\n",
      "7200 bottleneck files created.\n",
      "7300 bottleneck files created.\n",
      "7400 bottleneck files created.\n",
      "7500 bottleneck files created.\n",
      "7600 bottleneck files created.\n",
      "7700 bottleneck files created.\n",
      "7800 bottleneck files created.\n",
      "7900 bottleneck files created.\n",
      "8000 bottleneck files created.\n",
      "8100 bottleneck files created.\n",
      "8200 bottleneck files created.\n",
      "8300 bottleneck files created.\n",
      "8400 bottleneck files created.\n",
      "8500 bottleneck files created.\n",
      "8600 bottleneck files created.\n",
      "8700 bottleneck files created.\n",
      "8800 bottleneck files created.\n",
      "8900 bottleneck files created.\n",
      "9000 bottleneck files created.\n",
      "9100 bottleneck files created.\n",
      "9200 bottleneck files created.\n",
      "9300 bottleneck files created.\n",
      "9400 bottleneck files created.\n",
      "9500 bottleneck files created.\n",
      "9600 bottleneck files created.\n",
      "9700 bottleneck files created.\n",
      "9800 bottleneck files created.\n",
      "9900 bottleneck files created.\n",
      "10000 bottleneck files created.\n",
      "10100 bottleneck files created.\n",
      "10200 bottleneck files created.\n",
      "10300 bottleneck files created.\n",
      "10400 bottleneck files created.\n",
      "10500 bottleneck files created.\n",
      "10600 bottleneck files created.\n",
      "10700 bottleneck files created.\n",
      "10800 bottleneck files created.\n",
      "10900 bottleneck files created.\n",
      "11000 bottleneck files created.\n",
      "11100 bottleneck files created.\n",
      "11200 bottleneck files created.\n",
      "11300 bottleneck files created.\n",
      "11400 bottleneck files created.\n",
      "11500 bottleneck files created.\n",
      "11600 bottleneck files created.\n",
      "11700 bottleneck files created.\n",
      "11800 bottleneck files created.\n",
      "11900 bottleneck files created.\n",
      "12000 bottleneck files created.\n",
      "12100 bottleneck files created.\n",
      "12200 bottleneck files created.\n",
      "12300 bottleneck files created.\n",
      "12400 bottleneck files created.\n",
      "12500 bottleneck files created.\n",
      "12600 bottleneck files created.\n",
      "12700 bottleneck files created.\n",
      "12800 bottleneck files created.\n",
      "12900 bottleneck files created.\n",
      "13000 bottleneck files created.\n",
      "13100 bottleneck files created.\n",
      "13200 bottleneck files created.\n",
      "13300 bottleneck files created.\n",
      "13400 bottleneck files created.\n",
      "13500 bottleneck files created.\n",
      "13600 bottleneck files created.\n",
      "13700 bottleneck files created.\n",
      "13800 bottleneck files created.\n",
      "13900 bottleneck files created.\n",
      "14000 bottleneck files created.\n",
      "14100 bottleneck files created.\n",
      "14200 bottleneck files created.\n",
      "14300 bottleneck files created.\n",
      "14400 bottleneck files created.\n",
      "14500 bottleneck files created.\n",
      "14600 bottleneck files created.\n",
      "14700 bottleneck files created.\n",
      "14800 bottleneck files created.\n",
      "14900 bottleneck files created.\n",
      "15000 bottleneck files created.\n",
      "15100 bottleneck files created.\n",
      "15200 bottleneck files created.\n",
      "15300 bottleneck files created.\n",
      "15400 bottleneck files created.\n",
      "15500 bottleneck files created.\n",
      "15600 bottleneck files created.\n",
      "15700 bottleneck files created.\n",
      "15800 bottleneck files created.\n",
      "15900 bottleneck files created.\n",
      "16000 bottleneck files created.\n",
      "16100 bottleneck files created.\n",
      "WARNING:tensorflow:From /Users/Boulderc30/Desktop/Cavey-Final/image_retraining/retrain.py:768: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "INFO:tensorflow:Froze 2 variables.\n",
      "Converted 2 variables to const ops.\n",
      "2018-05-09 21:54:50.014115: Step 0: Train accuracy = 7.0%\n",
      "2018-05-09 21:54:50.014233: Step 0: Cross entropy = 3.425467\n",
      "2018-05-09 21:54:52.185662: Step 0: Validation accuracy = 2.0% (N=100)\n",
      "2018-05-09 21:54:52.912379: Step 10: Train accuracy = 35.0%\n",
      "2018-05-09 21:54:52.912475: Step 10: Cross entropy = 2.803937\n",
      "2018-05-09 21:54:52.985880: Step 10: Validation accuracy = 8.0% (N=100)\n",
      "2018-05-09 21:54:53.706991: Step 20: Train accuracy = 30.0%\n",
      "2018-05-09 21:54:53.707109: Step 20: Cross entropy = 2.562871\n",
      "2018-05-09 21:54:53.781205: Step 20: Validation accuracy = 12.0% (N=100)\n",
      "2018-05-09 21:54:54.517050: Step 30: Train accuracy = 56.0%\n",
      "2018-05-09 21:54:54.517143: Step 30: Cross entropy = 1.914347\n",
      "2018-05-09 21:54:54.590341: Step 30: Validation accuracy = 18.0% (N=100)\n",
      "2018-05-09 21:54:55.316313: Step 40: Train accuracy = 43.0%\n",
      "2018-05-09 21:54:55.316409: Step 40: Cross entropy = 2.117825\n",
      "2018-05-09 21:54:55.386321: Step 40: Validation accuracy = 19.0% (N=100)\n",
      "2018-05-09 21:54:56.109348: Step 50: Train accuracy = 67.0%\n",
      "2018-05-09 21:54:56.109459: Step 50: Cross entropy = 1.757222\n",
      "2018-05-09 21:54:56.181101: Step 50: Validation accuracy = 25.0% (N=100)\n",
      "2018-05-09 21:54:56.898882: Step 60: Train accuracy = 69.0%\n",
      "2018-05-09 21:54:56.898977: Step 60: Cross entropy = 1.603872\n",
      "2018-05-09 21:54:56.971801: Step 60: Validation accuracy = 24.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-09 21:54:57.732789: Step 70: Train accuracy = 58.0%\n",
      "2018-05-09 21:54:57.732884: Step 70: Cross entropy = 1.709345\n",
      "2018-05-09 21:54:57.807184: Step 70: Validation accuracy = 23.0% (N=100)\n",
      "2018-05-09 21:54:58.517781: Step 80: Train accuracy = 62.0%\n",
      "2018-05-09 21:54:58.517882: Step 80: Cross entropy = 1.554101\n",
      "2018-05-09 21:54:58.587216: Step 80: Validation accuracy = 24.0% (N=100)\n",
      "2018-05-09 21:54:59.313085: Step 90: Train accuracy = 70.0%\n",
      "2018-05-09 21:54:59.313186: Step 90: Cross entropy = 1.511660\n",
      "2018-05-09 21:54:59.384537: Step 90: Validation accuracy = 22.0% (N=100)\n",
      "2018-05-09 21:55:00.153461: Step 100: Train accuracy = 76.0%\n",
      "2018-05-09 21:55:00.153569: Step 100: Cross entropy = 1.223099\n",
      "2018-05-09 21:55:00.227718: Step 100: Validation accuracy = 31.0% (N=100)\n",
      "2018-05-09 21:55:01.010840: Step 110: Train accuracy = 74.0%\n",
      "2018-05-09 21:55:01.010939: Step 110: Cross entropy = 1.239083\n",
      "2018-05-09 21:55:01.093067: Step 110: Validation accuracy = 35.0% (N=100)\n",
      "2018-05-09 21:55:01.906559: Step 120: Train accuracy = 76.0%\n",
      "2018-05-09 21:55:01.906658: Step 120: Cross entropy = 1.173964\n",
      "2018-05-09 21:55:01.983014: Step 120: Validation accuracy = 26.0% (N=100)\n",
      "2018-05-09 21:55:02.730643: Step 130: Train accuracy = 71.0%\n",
      "2018-05-09 21:55:02.730741: Step 130: Cross entropy = 1.299472\n",
      "2018-05-09 21:55:02.805942: Step 130: Validation accuracy = 33.0% (N=100)\n",
      "2018-05-09 21:55:03.547700: Step 140: Train accuracy = 77.0%\n",
      "2018-05-09 21:55:03.547799: Step 140: Cross entropy = 1.067749\n",
      "2018-05-09 21:55:03.624203: Step 140: Validation accuracy = 28.0% (N=100)\n",
      "2018-05-09 21:55:04.379327: Step 150: Train accuracy = 71.0%\n",
      "2018-05-09 21:55:04.379440: Step 150: Cross entropy = 1.170865\n",
      "2018-05-09 21:55:04.453301: Step 150: Validation accuracy = 30.0% (N=100)\n",
      "2018-05-09 21:55:05.174139: Step 160: Train accuracy = 78.0%\n",
      "2018-05-09 21:55:05.174235: Step 160: Cross entropy = 1.219733\n",
      "2018-05-09 21:55:05.243459: Step 160: Validation accuracy = 22.0% (N=100)\n",
      "2018-05-09 21:55:05.990370: Step 170: Train accuracy = 78.0%\n",
      "2018-05-09 21:55:05.990470: Step 170: Cross entropy = 1.125001\n",
      "2018-05-09 21:55:06.066206: Step 170: Validation accuracy = 27.0% (N=100)\n",
      "2018-05-09 21:55:06.831419: Step 180: Train accuracy = 70.0%\n",
      "2018-05-09 21:55:06.831514: Step 180: Cross entropy = 1.157144\n",
      "2018-05-09 21:55:06.906819: Step 180: Validation accuracy = 28.0% (N=100)\n",
      "2018-05-09 21:55:07.666107: Step 190: Train accuracy = 78.0%\n",
      "2018-05-09 21:55:07.666205: Step 190: Cross entropy = 0.949429\n",
      "2018-05-09 21:55:07.742656: Step 190: Validation accuracy = 25.0% (N=100)\n",
      "2018-05-09 21:55:08.515095: Step 200: Train accuracy = 76.0%\n",
      "2018-05-09 21:55:08.515192: Step 200: Cross entropy = 1.103053\n",
      "2018-05-09 21:55:08.588250: Step 200: Validation accuracy = 29.0% (N=100)\n",
      "2018-05-09 21:55:09.328534: Step 210: Train accuracy = 79.0%\n",
      "2018-05-09 21:55:09.328630: Step 210: Cross entropy = 1.091656\n",
      "2018-05-09 21:55:09.402457: Step 210: Validation accuracy = 29.0% (N=100)\n",
      "2018-05-09 21:55:10.123173: Step 220: Train accuracy = 84.0%\n",
      "2018-05-09 21:55:10.123272: Step 220: Cross entropy = 0.950678\n",
      "2018-05-09 21:55:10.196371: Step 220: Validation accuracy = 24.0% (N=100)\n",
      "2018-05-09 21:55:10.961465: Step 230: Train accuracy = 85.0%\n",
      "2018-05-09 21:55:10.961572: Step 230: Cross entropy = 0.892073\n",
      "2018-05-09 21:55:11.034623: Step 230: Validation accuracy = 29.0% (N=100)\n",
      "2018-05-09 21:55:11.770820: Step 240: Train accuracy = 81.0%\n",
      "2018-05-09 21:55:11.770916: Step 240: Cross entropy = 0.852846\n",
      "2018-05-09 21:55:11.842339: Step 240: Validation accuracy = 29.0% (N=100)\n",
      "2018-05-09 21:55:12.576750: Step 250: Train accuracy = 81.0%\n",
      "2018-05-09 21:55:12.576852: Step 250: Cross entropy = 0.910558\n",
      "2018-05-09 21:55:12.646950: Step 250: Validation accuracy = 31.0% (N=100)\n",
      "2018-05-09 21:55:13.360707: Step 260: Train accuracy = 88.0%\n",
      "2018-05-09 21:55:13.360805: Step 260: Cross entropy = 0.748330\n",
      "2018-05-09 21:55:13.431895: Step 260: Validation accuracy = 31.0% (N=100)\n",
      "2018-05-09 21:55:14.150219: Step 270: Train accuracy = 81.0%\n",
      "2018-05-09 21:55:14.150312: Step 270: Cross entropy = 0.969966\n",
      "2018-05-09 21:55:14.223682: Step 270: Validation accuracy = 23.0% (N=100)\n",
      "2018-05-09 21:55:14.944152: Step 280: Train accuracy = 85.0%\n",
      "2018-05-09 21:55:14.944249: Step 280: Cross entropy = 0.802914\n",
      "2018-05-09 21:55:15.017025: Step 280: Validation accuracy = 28.0% (N=100)\n",
      "2018-05-09 21:55:15.731191: Step 290: Train accuracy = 83.0%\n",
      "2018-05-09 21:55:15.731290: Step 290: Cross entropy = 0.780704\n",
      "2018-05-09 21:55:15.801296: Step 290: Validation accuracy = 37.0% (N=100)\n",
      "2018-05-09 21:55:16.518766: Step 300: Train accuracy = 85.0%\n",
      "2018-05-09 21:55:16.518861: Step 300: Cross entropy = 0.718913\n",
      "2018-05-09 21:55:16.590765: Step 300: Validation accuracy = 32.0% (N=100)\n",
      "2018-05-09 21:55:17.325075: Step 310: Train accuracy = 84.0%\n",
      "2018-05-09 21:55:17.325172: Step 310: Cross entropy = 0.619532\n",
      "2018-05-09 21:55:17.400850: Step 310: Validation accuracy = 31.0% (N=100)\n",
      "2018-05-09 21:55:18.131586: Step 320: Train accuracy = 91.0%\n",
      "2018-05-09 21:55:18.131687: Step 320: Cross entropy = 0.627495\n",
      "2018-05-09 21:55:18.201807: Step 320: Validation accuracy = 25.0% (N=100)\n",
      "2018-05-09 21:55:18.945061: Step 330: Train accuracy = 91.0%\n",
      "2018-05-09 21:55:18.945159: Step 330: Cross entropy = 0.694839\n",
      "2018-05-09 21:55:19.017765: Step 330: Validation accuracy = 34.0% (N=100)\n",
      "2018-05-09 21:55:19.762047: Step 340: Train accuracy = 90.0%\n",
      "2018-05-09 21:55:19.762148: Step 340: Cross entropy = 0.631096\n",
      "2018-05-09 21:55:19.836323: Step 340: Validation accuracy = 31.0% (N=100)\n",
      "2018-05-09 21:55:20.569871: Step 350: Train accuracy = 83.0%\n",
      "2018-05-09 21:55:20.569968: Step 350: Cross entropy = 0.704234\n",
      "2018-05-09 21:55:20.643287: Step 350: Validation accuracy = 30.0% (N=100)\n",
      "2018-05-09 21:55:21.384833: Step 360: Train accuracy = 88.0%\n",
      "2018-05-09 21:55:21.384930: Step 360: Cross entropy = 0.635708\n",
      "2018-05-09 21:55:21.457303: Step 360: Validation accuracy = 37.0% (N=100)\n",
      "2018-05-09 21:55:22.189484: Step 370: Train accuracy = 87.0%\n",
      "2018-05-09 21:55:22.189580: Step 370: Cross entropy = 0.617853\n",
      "2018-05-09 21:55:22.260903: Step 370: Validation accuracy = 33.0% (N=100)\n",
      "2018-05-09 21:55:23.004272: Step 380: Train accuracy = 84.0%\n",
      "2018-05-09 21:55:23.004366: Step 380: Cross entropy = 0.677581\n",
      "2018-05-09 21:55:23.076616: Step 380: Validation accuracy = 25.0% (N=100)\n",
      "2018-05-09 21:55:23.788622: Step 390: Train accuracy = 92.0%\n",
      "2018-05-09 21:55:23.788720: Step 390: Cross entropy = 0.579695\n",
      "2018-05-09 21:55:23.859189: Step 390: Validation accuracy = 19.0% (N=100)\n",
      "2018-05-09 21:55:24.577112: Step 400: Train accuracy = 91.0%\n",
      "2018-05-09 21:55:24.577208: Step 400: Cross entropy = 0.587842\n",
      "2018-05-09 21:55:24.650270: Step 400: Validation accuracy = 27.0% (N=100)\n",
      "2018-05-09 21:55:25.384621: Step 410: Train accuracy = 89.0%\n",
      "2018-05-09 21:55:25.384718: Step 410: Cross entropy = 0.641758\n",
      "2018-05-09 21:55:25.454657: Step 410: Validation accuracy = 26.0% (N=100)\n",
      "2018-05-09 21:55:26.169707: Step 420: Train accuracy = 91.0%\n",
      "2018-05-09 21:55:26.169805: Step 420: Cross entropy = 0.668076\n",
      "2018-05-09 21:55:26.240962: Step 420: Validation accuracy = 32.0% (N=100)\n",
      "2018-05-09 21:55:26.964482: Step 430: Train accuracy = 88.0%\n",
      "2018-05-09 21:55:26.964581: Step 430: Cross entropy = 0.583579\n",
      "2018-05-09 21:55:27.037444: Step 430: Validation accuracy = 27.0% (N=100)\n",
      "2018-05-09 21:55:27.756652: Step 440: Train accuracy = 92.0%\n",
      "2018-05-09 21:55:27.756782: Step 440: Cross entropy = 0.545724\n",
      "2018-05-09 21:55:27.826819: Step 440: Validation accuracy = 25.0% (N=100)\n",
      "2018-05-09 21:55:28.559921: Step 450: Train accuracy = 88.0%\n",
      "2018-05-09 21:55:28.560017: Step 450: Cross entropy = 0.649234\n",
      "2018-05-09 21:55:28.632507: Step 450: Validation accuracy = 28.0% (N=100)\n",
      "2018-05-09 21:55:29.401242: Step 460: Train accuracy = 89.0%\n",
      "2018-05-09 21:55:29.401339: Step 460: Cross entropy = 0.635824\n",
      "2018-05-09 21:55:29.475471: Step 460: Validation accuracy = 31.0% (N=100)\n",
      "2018-05-09 21:55:30.212263: Step 470: Train accuracy = 92.0%\n",
      "2018-05-09 21:55:30.212357: Step 470: Cross entropy = 0.527942\n",
      "2018-05-09 21:55:30.285798: Step 470: Validation accuracy = 34.0% (N=100)\n",
      "2018-05-09 21:55:31.038287: Step 480: Train accuracy = 86.0%\n",
      "2018-05-09 21:55:31.038383: Step 480: Cross entropy = 0.720948\n",
      "2018-05-09 21:55:31.113176: Step 480: Validation accuracy = 28.0% (N=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-09 21:55:31.910464: Step 490: Train accuracy = 89.0%\n",
      "2018-05-09 21:55:31.910564: Step 490: Cross entropy = 0.532923\n",
      "2018-05-09 21:55:31.987616: Step 490: Validation accuracy = 32.0% (N=100)\n",
      "2018-05-09 21:55:32.749120: Step 500: Train accuracy = 94.0%\n",
      "2018-05-09 21:55:32.749214: Step 500: Cross entropy = 0.516827\n",
      "2018-05-09 21:55:32.818691: Step 500: Validation accuracy = 37.0% (N=100)\n",
      "2018-05-09 21:55:33.571770: Step 510: Train accuracy = 89.0%\n",
      "2018-05-09 21:55:33.571866: Step 510: Cross entropy = 0.554540\n",
      "2018-05-09 21:55:33.647941: Step 510: Validation accuracy = 30.0% (N=100)\n",
      "2018-05-09 21:55:34.390950: Step 520: Train accuracy = 91.0%\n",
      "2018-05-09 21:55:34.391051: Step 520: Cross entropy = 0.515003\n",
      "2018-05-09 21:55:34.462442: Step 520: Validation accuracy = 33.0% (N=100)\n",
      "2018-05-09 21:55:35.191483: Step 530: Train accuracy = 90.0%\n",
      "2018-05-09 21:55:35.191581: Step 530: Cross entropy = 0.558828\n",
      "2018-05-09 21:55:35.263258: Step 530: Validation accuracy = 25.0% (N=100)\n",
      "2018-05-09 21:55:35.988050: Step 540: Train accuracy = 94.0%\n",
      "2018-05-09 21:55:35.988165: Step 540: Cross entropy = 0.479520\n",
      "2018-05-09 21:55:36.061714: Step 540: Validation accuracy = 33.0% (N=100)\n",
      "2018-05-09 21:55:36.848429: Step 550: Train accuracy = 96.0%\n",
      "2018-05-09 21:55:36.848532: Step 550: Cross entropy = 0.455585\n",
      "2018-05-09 21:55:36.943339: Step 550: Validation accuracy = 28.0% (N=100)\n",
      "2018-05-09 21:55:37.771395: Step 560: Train accuracy = 93.0%\n",
      "2018-05-09 21:55:37.771490: Step 560: Cross entropy = 0.517367\n",
      "2018-05-09 21:55:37.841511: Step 560: Validation accuracy = 30.0% (N=100)\n",
      "2018-05-09 21:55:38.592355: Step 570: Train accuracy = 94.0%\n",
      "2018-05-09 21:55:38.592488: Step 570: Cross entropy = 0.517889\n",
      "2018-05-09 21:55:38.668044: Step 570: Validation accuracy = 30.0% (N=100)\n",
      "2018-05-09 21:55:39.408208: Step 580: Train accuracy = 90.0%\n",
      "2018-05-09 21:55:39.408304: Step 580: Cross entropy = 0.523357\n",
      "2018-05-09 21:55:39.479185: Step 580: Validation accuracy = 26.0% (N=100)\n",
      "2018-05-09 21:55:40.238188: Step 590: Train accuracy = 95.0%\n",
      "2018-05-09 21:55:40.238320: Step 590: Cross entropy = 0.381754\n",
      "2018-05-09 21:55:40.311652: Step 590: Validation accuracy = 35.0% (N=100)\n",
      "2018-05-09 21:55:41.032242: Step 600: Train accuracy = 92.0%\n",
      "2018-05-09 21:55:41.032336: Step 600: Cross entropy = 0.428031\n",
      "2018-05-09 21:55:41.106012: Step 600: Validation accuracy = 32.0% (N=100)\n",
      "2018-05-09 21:55:41.828822: Step 610: Train accuracy = 96.0%\n",
      "2018-05-09 21:55:41.828923: Step 610: Cross entropy = 0.345284\n",
      "2018-05-09 21:55:41.898516: Step 610: Validation accuracy = 24.0% (N=100)\n",
      "2018-05-09 21:55:42.734545: Step 620: Train accuracy = 91.0%\n",
      "2018-05-09 21:55:42.734643: Step 620: Cross entropy = 0.511797\n",
      "2018-05-09 21:55:42.807536: Step 620: Validation accuracy = 26.0% (N=100)\n",
      "2018-05-09 21:55:43.540756: Step 630: Train accuracy = 93.0%\n",
      "2018-05-09 21:55:43.540852: Step 630: Cross entropy = 0.428674\n",
      "2018-05-09 21:55:43.615129: Step 630: Validation accuracy = 32.0% (N=100)\n",
      "2018-05-09 21:55:44.390486: Step 640: Train accuracy = 95.0%\n",
      "2018-05-09 21:55:44.390583: Step 640: Cross entropy = 0.491608\n",
      "2018-05-09 21:55:44.466722: Step 640: Validation accuracy = 32.0% (N=100)\n",
      "2018-05-09 21:55:45.234424: Step 650: Train accuracy = 96.0%\n",
      "2018-05-09 21:55:45.234519: Step 650: Cross entropy = 0.347488\n",
      "2018-05-09 21:55:45.307132: Step 650: Validation accuracy = 30.0% (N=100)\n",
      "2018-05-09 21:55:46.035226: Step 660: Train accuracy = 91.0%\n",
      "2018-05-09 21:55:46.035327: Step 660: Cross entropy = 0.503090\n",
      "2018-05-09 21:55:46.106451: Step 660: Validation accuracy = 33.0% (N=100)\n",
      "2018-05-09 21:55:46.864519: Step 670: Train accuracy = 91.0%\n",
      "2018-05-09 21:55:46.864619: Step 670: Cross entropy = 0.456924\n",
      "2018-05-09 21:55:46.940642: Step 670: Validation accuracy = 31.0% (N=100)\n",
      "2018-05-09 21:55:47.692719: Step 680: Train accuracy = 92.0%\n",
      "2018-05-09 21:55:47.692815: Step 680: Cross entropy = 0.446003\n",
      "2018-05-09 21:55:47.767987: Step 680: Validation accuracy = 28.0% (N=100)\n",
      "2018-05-09 21:55:48.516897: Step 690: Train accuracy = 90.0%\n",
      "2018-05-09 21:55:48.516994: Step 690: Cross entropy = 0.467426\n",
      "2018-05-09 21:55:48.591658: Step 690: Validation accuracy = 30.0% (N=100)\n",
      "2018-05-09 21:55:49.256140: Step 699: Train accuracy = 92.0%\n",
      "2018-05-09 21:55:49.256241: Step 699: Cross entropy = 0.459040\n",
      "2018-05-09 21:55:49.329858: Step 699: Validation accuracy = 34.0% (N=100)\n",
      "Final test accuracy = 32.0% (N=200)\n",
      "=== MISCLASSIFIED TEST IMAGES ===\n",
      "                                         images_resized/GMC/009855.jpg  nissan\n",
      "1\n",
      "                                       images_resized/Chevy/005511.jpg  cadillac\n",
      "2\n",
      "                                       images_resized/Tesla/015238.jpg  audi\n",
      "3\n",
      "                                       images_resized/Volvo/016033.jpg  gmc\n",
      "4\n",
      "                                      images_resized/Fisker/008566.jpg  mercedes benz\n",
      "5\n",
      "                                        images_resized/Mini/012922.jpg  audi\n",
      "6\n",
      "                                      images_resized/Suzuki/014997.jpg  mercedes benz\n",
      "7\n",
      "                                       images_resized/Tesla/015184.jpg  rolls royce\n",
      "8\n",
      "                                  images_resized/Mitsubishi/013728.jpg  buick\n",
      "9\n",
      "                                       images_resized/Volvo/015854.jpg  suzuki\n",
      "10\n",
      "                                        images_resized/Jeep/012142.jpg  gmc\n",
      "11\n",
      "                                       images_resized/Tesla/015213.jpg  audi\n",
      "12\n",
      "                                     images_resized/Bugatti/003673.jpg  lamborghini\n",
      "13\n",
      "                                         images_resized/Geo/010139.jpg  chevy\n",
      "14\n",
      "                                      images_resized/Jaguar/011834.jpg  fisker\n",
      "15\n",
      "                                     images_resized/Lincoln/012801.jpg  chevy\n",
      "16\n",
      "                                  images_resized/Mitsubishi/013701.jpg  audi\n",
      "17\n",
      "                                       images_resized/Isuzu/011717.jpg  gmc\n",
      "18\n",
      "                                         images_resized/GMC/010033.jpg  hyundai\n",
      "19\n",
      "                                       images_resized/Scion/014634.jpg  bentley\n",
      "20\n",
      "                                  images_resized/Mitsubishi/013670.jpg  cadillac\n",
      "21\n",
      "                                      images_resized/Daewoo/006706.jpg  suzuki\n",
      "22\n",
      "                                    images_resized/Infiniti/011649.jpg  toyota\n",
      "23\n",
      "                                       images_resized/Isuzu/011683.jpg  ford\n",
      "24\n",
      "                                       images_resized/Chevy/004629.jpg  ferrari\n",
      "25\n",
      "                                     images_resized/Bentley/003246.jpg  volvo\n",
      "26\n",
      "                                   images_resized/Chrystler/006113.jpg  ford\n",
      "27\n",
      "                                      images_resized/Fisker/008561.jpg  cadillac\n",
      "28\n",
      "                                      images_resized/Fisker/008523.jpg  ferrari\n",
      "29\n",
      "                                       images_resized/Tesla/015201.jpg  audi\n",
      "30\n",
      "                                      images_resized/Nissan/014084.jpg  audi\n",
      "31\n",
      "                                      images_resized/Daewoo/006636.jpg  nissan\n",
      "32\n",
      "                                       images_resized/Honda/010543.jpg  cadillac\n",
      "33\n",
      "                                      images_resized/Hummer/000053.jpg  jeep\n",
      "34\n",
      "                                       images_resized/Acura/000303.jpg  mercedes benz\n",
      "35\n",
      "                                     images_resized/Bugatti/003631.jpg  nissan\n",
      "36\n",
      "                                       images_resized/Tesla/015184.jpg  rolls royce\n",
      "37\n",
      "                                images_resized/Aston_Martin/000780.jpg  chrystler\n",
      "38\n",
      "                                       images_resized/Chevy/005772.jpg  cadillac\n",
      "39\n",
      "                                     images_resized/Lincoln/012837.jpg  cadillac\n",
      "40\n",
      "                                      images_resized/Nissan/013894.jpg  gmc\n",
      "41\n",
      "                                     images_resized/Porsche/014190.jpg  hyundai\n",
      "42\n",
      "                                images_resized/Aston_Martin/000870.jpg  ferrari\n",
      "43\n",
      "                                  images_resized/Mitsubishi/013728.jpg  buick\n",
      "44\n",
      "                                     images_resized/Maybach/012930.jpg  nissan\n",
      "45\n",
      "                                     images_resized/Bentley/003571.jpg  mercedes benz\n",
      "46\n",
      "                                     images_resized/Hyundai/011389.jpg  land rover\n",
      "47\n",
      "                                        images_resized/Mini/012880.jpg  rolls royce\n",
      "48\n",
      "                                      images_resized/Jaguar/011796.jpg  aston martin\n",
      "49\n",
      "                                      images_resized/Nissan/013989.jpg  buick\n",
      "50\n",
      "                                  images_resized/Volkswagen/015824.jpg  hyundai\n",
      "51\n",
      "                                     images_resized/Bugatti/003673.jpg  lamborghini\n",
      "52\n",
      "                                      images_resized/Spyker/014693.jpg  lamborghini\n",
      "53\n",
      "                                       images_resized/Dodge/006720.jpg  gmc\n",
      "54\n",
      "                                        images_resized/Fiat/008165.jpg  chrystler\n",
      "55\n",
      "                                     images_resized/Bentley/003479.jpg  audi\n",
      "56\n",
      "                                      images_resized/Suzuki/014985.jpg  buick\n",
      "57\n",
      "                                       images_resized/Buick/004045.jpg  cadillac\n",
      "58\n",
      "                                       images_resized/Volvo/016082.jpg  ford\n",
      "59\n",
      "                                  images_resized/Volkswagen/015632.jpg  audi\n",
      "60\n",
      "                                   images_resized/Chrystler/008024.jpg  mercedes benz\n",
      "61\n",
      "                                      images_resized/Toyota/015308.jpg  suzuki\n",
      "62\n",
      "                                     images_resized/Maybach/012941.jpg  audi\n",
      "63\n",
      "                                  images_resized/Volkswagen/015660.jpg  scion\n",
      "64\n",
      "                                       images_resized/Volvo/015969.jpg  volkswagen\n",
      "65\n",
      "                                       images_resized/Tesla/015201.jpg  audi\n",
      "66\n",
      "                                       images_resized/Mazda/013046.jpg  infiniti\n",
      "67\n",
      "                                        images_resized/Fiat/008074.jpg  mercedes benz\n",
      "68\n",
      "                                      images_resized/Daewoo/006675.jpg  cadillac\n",
      "69\n",
      "                                  images_resized/Volkswagen/015662.jpg  honda\n",
      "70\n",
      "                                         images_resized/GMC/009715.jpg  dodge\n",
      "71\n",
      "                                images_resized/Aston_Martin/000878.jpg  bentley\n",
      "72\n",
      "                                     images_resized/Lincoln/012804.jpg  chrystler\n",
      "73\n",
      "                                      images_resized/Spyker/014805.jpg  chrystler\n",
      "74\n",
      "                                       images_resized/Tesla/015203.jpg  hyundai\n",
      "75\n",
      "                                      images_resized/Hummer/010324.jpg  jeep\n",
      "76\n",
      "                                      images_resized/Fisker/008578.jpg  aston martin\n",
      "77\n",
      "                                      images_resized/Nissan/013907.jpg  dodge\n",
      "78\n",
      "                                       images_resized/Mazda/013030.jpg  isuzu\n",
      "79\n",
      "                                      images_resized/Jaguar/011771.jpg  aston martin\n",
      "80\n",
      "                                  images_resized/Mitsubishi/013738.jpg  dodge\n",
      "81\n",
      "                                     images_resized/Ferrari/008200.jpg  audi\n",
      "82\n",
      "                                        images_resized/Ford/008924.jpg  volkswagen\n",
      "83\n",
      "                                      images_resized/Fisker/008539.jpg  acura\n",
      "84\n",
      "                                       images_resized/Buick/003860.jpg  volvo\n",
      "85\n",
      "                                     images_resized/Ferrari/008314.jpg  nissan\n",
      "86\n",
      "                                      images_resized/Daewoo/006647.jpg  volkswagen\n",
      "87\n",
      "                                      images_resized/Nissan/013760.jpg  hyundai\n",
      "88\n",
      "                                 images_resized/Rolls_Royce/014576.jpg  bentley\n",
      "89\n",
      "                                        images_resized/Ford/008686.jpg  audi\n",
      "90\n",
      "                                      images_resized/Fisker/008528.jpg  rolls royce\n",
      "91\n",
      "                                       images_resized/Chevy/004518.jpg  audi\n",
      "92\n",
      "                                       images_resized/Tesla/015230.jpg  mercedes benz\n",
      "93\n",
      "                                      images_resized/Daewoo/006630.jpg  lincoln\n",
      "94\n",
      "                                       images_resized/Scion/014647.jpg  suzuki\n",
      "95\n",
      "                                 images_resized/Lamborghini/012556.jpg  spyker\n",
      "96\n",
      "                                     images_resized/Hyundai/010943.jpg  nissan\n",
      "97\n",
      "                                     images_resized/Ferrari/008486.jpg  mercedes benz\n",
      "98\n",
      "                                        images_resized/Jeep/012204.jpg  chrystler\n",
      "99\n",
      "                                     images_resized/Maybach/012952.jpg  volkswagen\n",
      "100\n",
      "                                        images_resized/Mini/012855.jpg  spyker\n",
      "101\n",
      "                                         images_resized/BMW/002335.jpg  buick\n",
      "102\n",
      "                                     images_resized/Porsche/014268.jpg  acura\n",
      "103\n",
      "                                       images_resized/Buick/003980.jpg  honda\n",
      "104\n",
      "                                     images_resized/Bugatti/003733.jpg  ferrari\n",
      "105\n",
      "                                     images_resized/Hyundai/010837.jpg  suzuki\n",
      "106\n",
      "                                    images_resized/Infiniti/011624.jpg  cadillac\n",
      "107\n",
      "                                      images_resized/Nissan/013931.jpg  volkswagen\n",
      "108\n",
      "                                        images_resized/Fiat/008081.jpg  bentley\n",
      "109\n",
      "                                       images_resized/Volvo/016037.jpg  suzuki\n",
      "110\n",
      "                                  images_resized/Mitsubishi/013692.jpg  cadillac\n",
      "111\n",
      "                                       images_resized/Tesla/015196.jpg  ferrari\n",
      "112\n",
      "                                       images_resized/Scion/014660.jpg  nissan\n",
      "113\n",
      "                                      images_resized/Spyker/014685.jpg  ferrari\n",
      "114\n",
      "                                  images_resized/Mitsubishi/013686.jpg  lincoln\n",
      "115\n",
      "                                       images_resized/Isuzu/011700.jpg  volvo\n",
      "116\n",
      "                                       images_resized/Acura/000550.jpg  toyota\n",
      "117\n",
      "                                    images_resized/Infiniti/011564.jpg  aston martin\n",
      "118\n",
      "                                       images_resized/Smart/016118.jpg  fiat\n",
      "119\n",
      "                                         images_resized/BMW/002604.jpg  buick\n",
      "120\n",
      "                                  images_resized/Mitsubishi/013727.jpg  audi\n",
      "121\n",
      "                                        images_resized/Mini/012912.jpg  hummer\n",
      "122\n",
      "                                       images_resized/Chevy/005797.jpg  bentley\n",
      "123\n",
      "                                     images_resized/Lincoln/012801.jpg  chevy\n",
      "124\n",
      "                                      images_resized/Jaguar/011764.jpg  ferrari\n",
      "125\n",
      "                                       images_resized/Acura/000164.jpg  mercedes benz\n",
      "126\n",
      "                                     images_resized/Porsche/014195.jpg  audi\n",
      "127\n",
      "                                     images_resized/Hyundai/011062.jpg  ferrari\n",
      "128\n",
      "                                     images_resized/Maybach/012943.jpg  jeep\n",
      "129\n",
      "                                     images_resized/Maybach/012970.jpg  audi\n",
      "130\n",
      "                                        images_resized/Fiat/008068.jpg  toyota\n",
      "131\n",
      "                                        images_resized/Audi/002010.jpg  bentley\n",
      "132\n",
      "                                      images_resized/Fisker/008554.jpg  mercedes benz\n",
      "133\n",
      "                                       images_resized/Honda/010388.jpg  suzuki\n",
      "134\n",
      "                                      images_resized/Jaguar/011757.jpg  bentley\n",
      "135\n",
      "                                       images_resized/Tesla/015183.jpg  ferrari\n",
      "136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Froze 2 variables.\n",
      "Converted 2 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "  # Setup the directory we'll write summaries to for TensorBoard\n",
    "  if tf.gfile.Exists(FLAGS.summaries_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.summaries_dir)\n",
    "  tf.gfile.MakeDirs(FLAGS.summaries_dir)\n",
    "\n",
    "  # Set up the pre-trained graph.\n",
    "  graph, bottleneck_tensor, jpeg_data_tensor, resized_image_tensor = (\n",
    "      retrain.create_inception_graph())\n",
    "\n",
    "  # Look at the folder structure, and create lists of all the images.\n",
    "  # This is why we use placeholder images when we reuse bottleneck files.\n",
    "  image_lists = retrain.create_image_lists(stored_images_resized, 80, 10)\n",
    "  class_count = len(image_lists.keys())\n",
    "  if class_count == 0:\n",
    "    raise Exception('No valid folders of images found at ' + FLAGS.image_dir)\n",
    "  if class_count == 1:\n",
    "    raise Exception('Only one valid folder of images found at ' + FLAGS.image_dir +\n",
    "          ' - multiple classes are needed for classification.')\n",
    "\n",
    "  with tf.Session(graph=graph) as sess:\n",
    "\n",
    "    # Calculate and cache bottleneck files based on the resized images\n",
    "    retrain.cache_bottlenecks(sess, image_lists, FLAGS.image_dir,\n",
    "                    FLAGS.bottleneck_dir, jpeg_data_tensor,\n",
    "                    bottleneck_tensor)\n",
    "\n",
    "    # Add the new layer that we'll be training.\n",
    "    (train_step, cross_entropy, bottleneck_input, ground_truth_input,\n",
    "     final_tensor) = retrain.add_final_training_ops(len(image_lists.keys()),\n",
    "                                            FLAGS.final_tensor_name,\n",
    "                                            bottleneck_tensor)\n",
    "\n",
    "    # Create the operations we need to evaluate the accuracy of our new layer.\n",
    "    evaluation_step, prediction = retrain.add_evaluation_step(\n",
    "        final_tensor, ground_truth_input)\n",
    "\n",
    "    # Merge all the summaries and write them out to the summaries_dir\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.summaries_dir + '/train',\n",
    "                                         sess.graph)\n",
    "\n",
    "    validation_writer = tf.summary.FileWriter(\n",
    "        FLAGS.summaries_dir + '/validation')\n",
    "\n",
    "    # Set up all our weights to their initial default values.\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Save the original graph, so we can compare results later!\n",
    "    output_graph_def = graph_util.convert_variables_to_constants(\n",
    "        sess, graph.as_graph_def(), [final_tensor_name])\n",
    "    with gfile.FastGFile(output_graph_orig, 'wb') as f:\n",
    "        f.write(output_graph_def.SerializeToString())\n",
    "\n",
    "    # Run the training!\n",
    "    for i in range(FLAGS.how_many_training_steps):\n",
    "\n",
    "      (train_bottlenecks, train_ground_truth, _) = retrain.get_random_cached_bottlenecks(\n",
    "             sess, image_lists, FLAGS.train_batch_size, 'training',\n",
    "             FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n",
    "             bottleneck_tensor)\n",
    "    \n",
    "      # Feed the bottlenecks and ground truth into the graph, and run a training\n",
    "      # step. Capture training summaries for TensorBoard with the `merged` op.\n",
    "      train_summary, _ = sess.run(\n",
    "          [merged, train_step],\n",
    "          feed_dict={bottleneck_input: train_bottlenecks,\n",
    "                     ground_truth_input: train_ground_truth})\n",
    "      train_writer.add_summary(train_summary, i)\n",
    "\n",
    "      # Every so often, print out how well the graph is training.\n",
    "      is_last_step = (i + 1 == FLAGS.how_many_training_steps)\n",
    "      if (i % FLAGS.eval_step_interval) == 0 or is_last_step:\n",
    "        train_accuracy, cross_entropy_value = sess.run(\n",
    "            [evaluation_step, cross_entropy],\n",
    "            feed_dict={bottleneck_input: train_bottlenecks,\n",
    "                       ground_truth_input: train_ground_truth})\n",
    "        print('%s: Step %d: Train accuracy = %.1f%%' % (datetime.now(), i,\n",
    "                                                        train_accuracy * 100))\n",
    "        print('%s: Step %d: Cross entropy = %f' % (datetime.now(), i,\n",
    "                                                   cross_entropy_value))\n",
    "        validation_bottlenecks, validation_ground_truth, _ = (\n",
    "            retrain.get_random_cached_bottlenecks(\n",
    "                sess, image_lists, FLAGS.validation_batch_size, 'validation',\n",
    "                FLAGS.bottleneck_dir, FLAGS.image_dir, jpeg_data_tensor,\n",
    "                bottleneck_tensor))\n",
    "        # Run a validation step and capture training summaries for TensorBoard\n",
    "        # with the `merged` op.\n",
    "        validation_summary, validation_accuracy = sess.run(\n",
    "            [merged, evaluation_step],\n",
    "            feed_dict={bottleneck_input: validation_bottlenecks,\n",
    "                       ground_truth_input: validation_ground_truth})\n",
    "        validation_writer.add_summary(validation_summary, i)\n",
    "        print('%s: Step %d: Validation accuracy = %.1f%% (N=%d)' %\n",
    "              (datetime.now(), i, validation_accuracy * 100,\n",
    "               len(validation_bottlenecks)))\n",
    "\n",
    "    # We've completed all our training, so run a final test evaluation on\n",
    "    # some new images we haven't used before.\n",
    "    test_bottlenecks, test_ground_truth, test_filenames = (\n",
    "        retrain.get_random_cached_bottlenecks(sess, image_lists, FLAGS.test_batch_size,\n",
    "                                      'testing', FLAGS.bottleneck_dir,\n",
    "                                      FLAGS.image_dir, jpeg_data_tensor,\n",
    "                                      bottleneck_tensor))\n",
    "    test_accuracy, predictions = sess.run(\n",
    "        [evaluation_step, prediction],\n",
    "        feed_dict={bottleneck_input: test_bottlenecks,\n",
    "                   ground_truth_input: test_ground_truth})\n",
    "    print('Final test accuracy = %.1f%% (N=%d)' % (\n",
    "        test_accuracy * 100, len(test_bottlenecks)))\n",
    "\n",
    "    if FLAGS.print_misclassified_test_images:\n",
    "      print('=== MISCLASSIFIED TEST IMAGES ===')\n",
    "      total = 0\n",
    "      for i, test_filename in enumerate(test_filenames):\n",
    "        if predictions[i] != test_ground_truth[i].argmax():\n",
    "          print('%70s  %s' % (test_filename, list(image_lists.keys())[predictions[i]]))\n",
    "          total += 1\n",
    "          print(total)\n",
    "\n",
    "    # Write out the trained graph and labels with the weights stored as\n",
    "    # constants.\n",
    "    output_graph_def = graph_util.convert_variables_to_constants(\n",
    "        sess, graph.as_graph_def(), [FLAGS.final_tensor_name])\n",
    "    with gfile.FastGFile(FLAGS.output_graph, 'wb') as f:\n",
    "      f.write(output_graph_def.SerializeToString())\n",
    "    with gfile.FastGFile(FLAGS.output_labels, 'w') as f:\n",
    "      f.write('\\n'.join(image_lists.keys()) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see in the above results in the classifications that were missed, it shows which car image, and the prediction. Here is where you can see a lot of interesting things about this data. Integers to the left are total number of incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "\n",
    "\n",
    "# First run:\n",
    "#### Training params\n",
    "architecture = 'inception_v3'  \n",
    "final_tensor_name = \"final_result\"  \n",
    "how_many_training_steps = 500  \n",
    "learning_rate = 0.08  \n",
    "testing_percentage = 10  \n",
    "validation_percentage = 10  \n",
    "eval_step_interval = 10  \n",
    "train_batch_size = 100  \n",
    "test_batch_size = -1  \n",
    "validation_batch_size = 100  \n",
    "print_misclassified_test_images = True  \n",
    "##### Final test accuracy = 24.1% (N=12910)  \n",
    "##### 9805 misclassified images (out of 16100)  \n",
    "\n",
    "# Second run:\n",
    "#### Training params\n",
    "architecture = 'inception_v3'  \n",
    "final_tensor_name = \"final_result\"  \n",
    "how_many_training_steps = 1000  \n",
    "learning_rate = 0.1  \n",
    "testing_percentage = 10  \n",
    "validation_percentage = 20  \n",
    "eval_step_interval = 10  \n",
    "train_batch_size = 100  \n",
    "test_batch_size = -1  \n",
    "validation_batch_size = 100  \n",
    "print_misclassified_test_images = True  \n",
    "##### Final test accuracy = 28.1% (N=12910)\n",
    "##### 9283 misclassified images (out of 12910)\n",
    "##### 3627 correctly classified images\n",
    "\n",
    "# Third run:\n",
    "#### Training params\n",
    "architecture = 'inception_v3'  \n",
    "final_tensor_name = \"final_result\"  \n",
    "how_many_training_steps = 1000  \n",
    "learning_rate = 0.2  \n",
    "testing_percentage = 10  \n",
    "validation_percentage = 20  \n",
    "eval_step_interval = 10  \n",
    "train_batch_size = 100  \n",
    "test_batch_size = -1  \n",
    "validation_batch_size = 100  \n",
    "print_misclassified_test_images = True  \n",
    "##### Final test accuracy = 31.2% (N=12910)\n",
    "##### 8879 misclassified images (out of 12910)\n",
    "##### 4031 correctly classified images\n",
    "\n",
    "# Best run:\n",
    "#### Training params\n",
    "architecture = 'inception_v3'  \n",
    "final_tensor_name = \"final_result\"  \n",
    "how_many_training_steps = 600  \n",
    "learning_rate = 0.2  \n",
    "testing_percentage = 50  \n",
    "validation_percentage = 50  \n",
    "eval_step_interval = 10  \n",
    "train_batch_size = 100  \n",
    "test_batch_size = 500  \n",
    "validation_batch_size = 100  \n",
    "print_misclassified_test_images = True  \n",
    "##### Final test accuracy = 40.2% (N=500)\n",
    "##### 290 misclassified images (out of 500)\n",
    "##### 210 correctly classified images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The final train accuracy was consistently ~99% after more than 800 training steps.\n",
    "The final test accuracy was anywhere from 30%-40%. This might seem low, but considering that there are **45** different classifications of car manufacturers, this is pretty good. I imagine that they'd all be pretty close to eachother, and the only thing seperating one car from another is very small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with graph=output_graph_orig.pb\n",
      "\n",
      "newTest/V/015866.jpg\n",
      "bentley (score = 0.02280)\n",
      "audi (score = 0.02280)\n",
      "jaguar (score = 0.02276)\n",
      "daewoo (score = 0.02271)\n",
      "fiat (score = 0.02268)\n",
      "newTest/V/015885.jpg\n",
      "mazda (score = 0.02276)\n",
      "fiat (score = 0.02273)\n",
      "chrystler (score = 0.02261)\n",
      "hyundai (score = 0.02260)\n",
      "tesla (score = 0.02260)\n",
      "newTest/V/015896.jpg\n",
      "tesla (score = 0.02277)\n",
      "mazda (score = 0.02267)\n",
      "ferrari (score = 0.02263)\n",
      "jaguar (score = 0.02260)\n",
      "bentley (score = 0.02257)\n",
      "newTest/V/015898.jpg\n",
      "dodge (score = 0.02278)\n",
      "porsche (score = 0.02274)\n",
      "bentley (score = 0.02271)\n",
      "audi (score = 0.02264)\n",
      "jaguar (score = 0.02259)\n",
      "\n",
      "Testing with graph=output_graph.pb\n",
      "\n",
      "newTest/V/015866.jpg\n",
      "honda (score = 0.22089)\n",
      "nissan (score = 0.14257)\n",
      "toyota (score = 0.10304)\n",
      "suzuki (score = 0.09888)\n",
      "cadillac (score = 0.07771)\n",
      "newTest/V/015885.jpg\n",
      "volkswagen (score = 0.15539)\n",
      "volvo (score = 0.13933)\n",
      "buick (score = 0.13528)\n",
      "audi (score = 0.09478)\n",
      "suzuki (score = 0.09172)\n",
      "newTest/V/015896.jpg\n",
      "scion (score = 0.17936)\n",
      "honda (score = 0.13444)\n",
      "gmc (score = 0.11133)\n",
      "chrystler (score = 0.09425)\n",
      "suzuki (score = 0.08480)\n",
      "newTest/V/015898.jpg\n",
      "volkswagen (score = 0.47680)\n",
      "nissan (score = 0.16898)\n",
      "scion (score = 0.04706)\n",
      "cadillac (score = 0.03613)\n",
      "jeep (score = 0.02463)\n"
     ]
    }
   ],
   "source": [
    "# Test with the test_images subdirs\n",
    "for graph in (output_graph_orig, output_graph):\n",
    "    print(\"\\nTesting with graph=%s\\n\" % graph)\n",
    "    for subdir in ('V'):\n",
    "        test_dir = os.path.join(test_images_dir, subdir)\n",
    "        for f in os.listdir(test_dir):\n",
    "            if f.endswith(JPEG_EXTENSIONS):\n",
    "                tf.reset_default_graph()\n",
    "                image = os.path.join(test_dir, f)\n",
    "                print(image)\n",
    "                %run image_retraining/label_image.py --image=$image --graph=$graph --labels=$output_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can see the point spread for a few images before/after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things that went wrong\n",
    "\n",
    "I originally started this project with a different idea in mind, I was going to use the code which Dr. Anderson provided for a Convolutional neural network from lecture 21. I quickly learned that building and validating my own model was a much bigger task than I originally thought. I began to search for an already trained model, that did its original task fairly well, and was just going to adapt it to my data. This was kind of the hard part, was finding a network that was used to solve a task that was remotely close to mine.\n",
    "\n",
    "After some searching, and recalling that Dr. Anderson warned us about pre-trained neural networks... I reluctantly found the only way to get this done on time was to use the Google Inception-V3 Transfer network with Tensorflow. Although this wasn't my ideal chioce, this was easier to do because I was able to use the knowledge of an already trained network that solved a previous problem, and fine tune it for my problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things I Thought were noteworthy\n",
    "A couple things I thought were really interesting:\n",
    "1. When the model got it's prediction wrong, it was interesting to note what was similar between the car image that was wrong, and what the model thought it was. Many times a Volvo was predicted to be a Volkswagen, a Chevy is a Dodge and visa versa. To me this shows the similarities in vehicle design are pretty apparent to this model, like Domestic vehicles, European vehicles, Exotic vehicles are always paired with eachother.\n",
    "\n",
    "2. Related to above, there was never a Ferrari predicted to be anything but an exotic car. Ferrari was only ever predicted to be a Lamborghini, Tesla, Jaguar and Spyker.\n",
    "\n",
    "3. The majority of cars that fell into a \"grey area\" (blocky, large cars) were predicted to be a Volkswagen\n",
    "\n",
    "4. Volvo and Volkswagen were the strongest link when guessed incorrectly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "What I would like to do next time is figure out how to record a \"second guess\" for the model. It would be interesting to see for instance, the top two guesses for each vehicle. I would also like to record the occurances of which cars that were predicted wrong, and compare them to what the prediction was. Seeing as I was able to achive ~40% correct classification, I would assume that the network I used was not the best suit for the purpose I was trying to apply it towards.\n",
    "\n",
    "I'm sure there is quite a few things that this model has learned that just doesn't apply to my car data, and that there is also many things that the model could benefit from if more car data was used in the original training.\n",
    "\n",
    "Reading about the Inception-V3 model, it appears that it's trained on 1000 different classes, but it is more general objects. I wanted to use multiple different Transfer learning networks and compare them. Especially if I could find one specifically for cars, that would be cool.\n",
    "\n",
    "I am a little bit satisfied with the Inception-V3 Transfer learning model. I originally thought it would do much better, but seeing as it was trained on general objects I am not suprised. If I chose to try and classify something else besides specific car manufactuerers, I would choose something that is a little more general. For example, is it a hotdog or not a hotdog? I feel like the dataset I chose was not very similar to the one the Inception-V3 was trained on. Unfortuneatly I didn't have time to test the various other architectures that Imagenet or Tensorflow might have available.\n",
    "\n",
    "Last but not least, I would have loved to graph more data. The complexity of trying to figure this all out by creating a network from scratch was difficult. I abandoned the idea after much reluctance, because I had to get the ball rolling on actually classifing things! (I really wanted to have a network built FOR cars). Overall I learned alot from this assignment alone, and will do the best I can in the future to not always jump to the easy \"pre-trained\" networks because of how this model has proved to be \"so-so\" in terms of prediction. Thank you!\n",
    "\n",
    "### Sources\n",
    "3D Object Representations for Fine-Grained Categorization\n",
    "Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei\n",
    "4th IEEE Workshop on 3D Representation and Recognition, at ICCV 2013 (3dRR-13). Sydney, Australia. Dec. 8, 2013.\n",
    "\n",
    "Cars dataset from Jonathan Krause at Stanford, Computer Science\n",
    "https://ai.stanford.edu/~jkrause/cars/car_dataset.html\n",
    "\n",
    "PowerAI Transfer Learning GitHub example\n",
    "https://github.com/IBM/powerai-transfer-learning?cm_sp=IBMCode-_-image-recognition-training-powerai-notebooks-_-Get-the-Code\n",
    "\n",
    "Franck Barillaud's GitHub [Franck Barillaud](https://github.com/fbarilla) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for file Cavey-Final.ipynb is 2211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Boulderc30/anaconda3/lib/python3.6/site-packages/IPython/nbformat.py:13: ShimWarning: The `IPython.nbformat` package has been deprecated since IPython 4.0. You should import from nbformat instead.\n",
      "  \"You should import from nbformat instead.\", ShimWarning)\n",
      "/Users/Boulderc30/anaconda3/lib/python3.6/site-packages/nbformat/current.py:19: UserWarning: nbformat.current is deprecated.\n",
      "\n",
      "- use nbformat for read/write/validate public API\n",
      "- use nbformat.vX directly to composing notebooks of a particular version\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from IPython.nbformat import current\n",
    "import glob\n",
    "nbfile = glob.glob('Cavey-Final.ipynb')\n",
    "if len(nbfile) > 1:\n",
    "    print('More than one ipynb file. Using the first one.  nbfile=', nbfile)\n",
    "with io.open(nbfile[0], 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "word_count = 0\n",
    "for cell in nb.worksheets[0].cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print('Word count for file', nbfile[0], 'is', word_count)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
